{
  
    
        "post0": {
            "title": "Develop Android app to park a camper level",
            "content": "Develop Android app to park a camper level . . In the blog Rotationssensor MPU-6050 mit WebGL am Raspberry Pi visualisieren . Der Raspberry Pi ist zu vielem in der Lage, so können einfach Rotations- und Beschleunigugnswerte mittels eines Sensors, wie dem MPU-6050, ausgelesen werden. Das Ergebnis sind jedoch einfache Zahlen, worunter man sich im Normalfall nicht all zu viel vorstellen wird. Jedoch ist es sehr einfach diese Zahlen zu visualisieren. Dies geht in modernen Browsern ganz einfach mittels WebGL, womit man 2D und 3D Objekte im Browser rendern kann. . Sensor MPU 6050 VNC: Remote access a Raspberry Pi . Setting up a VNC connection from phone to the raspi follow the instructions given at . https://magpi.raspberrypi.org/articles/vnc-raspberry-pi . Setting up a Raspberry Pi as a Wireless Access Point . Setting up the raspi as a wireless access point so that you can use VNC on a camping ground follow instructions given at https://www.raspberrypi.org/documentation/configuration/wireless/access-point.md This documentation assumes that we are using the standard 192.168.x.x IP addresses for our wireless network, so we will assign the server the IP address 192.168.4.1. . Note: Change default in sudo nano /etc/hostapd/hostapd.conf to something meaningful to you . ssid=NameOfNetwork # change this hw_mode=g channel=7 wmm_enabled=0 macaddr_acl=0 auth_algs=1 ignore_broadcast_ssid=0 wpa=2 wpa_passphrase=AardvarkBadgerHedgehog # change this . Measuring Rotation and acceleration with the Raspberry Pi . Follow instructions at MPU6050 (Accelerometer+Gyroscope) Interfacing with Raspberry Pi https://www.electronicwings.com/raspberry-pi/mpu6050-accelerometergyroscope-interfacing-with-raspberry-pi . Calculate pitch and roll . from MPU6050_9Axis_MotionApps41.h . uint8_t MPU6050::dmpGetYawPitchRoll(float data, Quaternion *q, VectorFloat *gravity) { // yaw: (about Z axis) data[0] = atan2(2q -&gt; xq -&gt; y - 2q -&gt; wq -&gt; z, 2q -&gt; wq -&gt; w + 2q -&gt; xq -&gt; x - 1); // pitch: (nose up/down, about Y axis) data[1] = atan(gravity -&gt; x / sqrt(gravity -&gt; ygravity -&gt; y + gravity -&gt; zgravity -&gt; z)); // roll: (tilt left/right, about X axis) data[2] = atan(gravity -&gt; y / sqrt(gravity -&gt; xgravity -&gt; x + gravity -&gt; z*gravity -&gt; z)); return 0; . The following output . Gyroscope ——– gyroscope_xout: -260 scaled: -2 gyroscope_yout: -154 scaled: -2 gyroscope_zout: 78 scaled: 0 . Accelerometer . acceleration_xout: -1048 scaled: -0.06396484375 acceleration_yout: -676 scaled: -0.041259765625 acceleration_zout: 16644 scaled: 1.01586914062 X Rotation: -2.32121150537 Y Rotation: 3.59994842011 . is created by [Measuring Rotation and acceleration with the Raspberry Pi](http://www.raspberrypirobotics.com/measuring-rotation-and-acceleration-with-the-raspberry-pi/) python #!/usr/bin/python import smbus import math # Register power_mgmt_1 = 0x6b power_mgmt_2 = 0x6c def read_byte(reg): return bus.read_byte_data(address, reg) def read_word(reg): h = bus.read_byte_data(address, reg) l = bus.read_byte_data(address, reg+1) value = (h &lt;&lt; 8) + l return value def read_word_2c(reg): val = read_word(reg) if (val &gt;= 0x8000): return -((65535 - val) + 1) else: return val def dist(a,b): return math.sqrt((a*a)+(b*b)) def get_y_rotation(x,y,z): radians = math.atan2(x, dist(y,z)) return -math.degrees(radians) def get_x_rotation(x,y,z): radians = math.atan2(y, dist(x,z)) return math.degrees(radians) bus = smbus.SMBus(1) # bus = smbus.SMBus(0) fuer Revision 1 address = 0x68 # via i2cdetect # Aktivieren, um das Modul ansprechen zu koennen bus.write_byte_data(address, power_mgmt_1, 0) print &quot;Gyroscope Sensor&quot; print &quot;--&quot; gyroscope_xout = read_word_2c(0x43) gyroscope_yout = read_word_2c(0x45) gyroscope_zout = read_word_2c(0x47) print &quot;gyroscope_xout: &quot;, (&quot;%5d&quot; % gyroscope_xout), &quot; scaled: &quot;, (gyroscope_xout / 131) print &quot;gyroscope_yout: &quot;, (&quot;%5d&quot; % gyroscope_yout), &quot; scaled: &quot;, (gyroscope_yout / 131) print &quot;gyroscope_zout: &quot;, (&quot;%5d&quot; % gyroscope_zout), &quot; scaled: &quot;, (gyroscope_zout / 131) print print &quot;Accelerometer Sensor&quot; print &quot;&quot; acceleration_xout = read_word_2c(0x3b) acceleration_yout = read_word_2c(0x3d) acceleration_zout = read_word_2c(0x3f) acceleration_xout_scaled = acceleration_xout / 16384.0 acceleration_yout_scaled = acceleration_yout / 16384.0 acceleration_zout_scaled = acceleration_zout / 16384.0 print &quot;acceleration_xout: &quot;, (&quot;%6d&quot; % acceleration_xout), &quot; scaled: &quot;, acceleration_xout_scaled print &quot;acceleration_yout: &quot;, (&quot;%6d&quot; % acceleration_yout), &quot; scaled: &quot;, acceleration_yout_scaled print &quot;acceleration_zout: &quot;, (&quot;%6d&quot; % acceleration_zout), &quot; scaled: &quot;, acceleration_zout_scaled print &quot;X Rotation: &quot; , get_x_rotation(acceleration_xout_scaled, acceleration_yout_scaled, acceleration_zout_scaled) print &quot;Y Rotation: &quot; , get_y_rotation(acceleration_xout_scaled, acceleration_yout_scaled, acceleration_zout_scaled) . Sending data to an HTTP server - get and post methods . Sending data to an HTTP server - get and post methods . MQTT . Eclipse Mosquitto™ An open source MQTT broker Mosquitto is highly portable and available for a wide range of platforms. For Raspberry Pi Mosquitto is available through the main repository. . Introduction to IoT: Build an MQTT Server Using Raspberry Pi . https://appcodelabs.com/introduction-to-iot-build-an-mqtt-server-using-raspberry-pi .",
            "url": "https://uwesterr.github.io/MlFastAiBlogNew/raspi/mpu6050/android/2020/03/11/MPU6050-Raspi-Arduino.html",
            "relUrl": "/raspi/mpu6050/android/2020/03/11/MPU6050-Raspi-Arduino.html",
            "date": " • Mar 11, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Train donkeycar with GPU support in Colab",
            "content": ". Credit . . Note: This notebook is based on https://colab.research.google.com/github/robocarstore/donkey-car-training-on-google-colab/blob/master/Donkey_Car_Training_using_Google_Colab.ipynb . Get necessary libraries . %tensorflow_version 1.13.1 import tensorflow from google.colab import drive from google.colab import files from IPython.display import Image import glob import shutil . . Step 1: Create environment . To train a neural network for the donkeycar we need a few components . install donkeycar | upload data via direct upload | mount Google drive | . | . . Note: Donkeycar at the time of writing in March 2020 uses Tensorflow 1.13, therefore version 1.xx is installed . print(tensorflow.__version__) . . Git Clone the donkeycar repository . Get the latest donkeycar version from GitHub . Note: The default branch is &quot;dev&quot;, however, the documentation is for the master branch. . !git clone https://github.com/autorope/donkeycar.git %cd /content/donkeycar !git checkout master . Cloning into &#39;donkeycar&#39;... remote: Enumerating objects: 12972, done. remote: Total 12972 (delta 0), reused 0 (delta 0), pack-reused 12972 Receiving objects: 100% (12972/12972), 67.74 MiB | 31.90 MiB/s, done. Resolving deltas: 100% (8193/8193), done. /content/donkeycar Branch &#39;master&#39; set up to track remote branch &#39;master&#39; from &#39;origin&#39;. Switched to a new branch &#39;master&#39; . Install donkey car . Different to the description at http://docs.donkeycar.com/guide/host_pc/setup_ubuntu/ we create no anaconda environment since the script is supposed to run on Colab which will delete the instance anyway once you disconnect the notebook. . !pip3 install -e .[pc] . Obtaining file:///content/donkeycar Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from donkeycar==3.1.2) (1.18.4) Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from donkeycar==3.1.2) (7.0.0) Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from donkeycar==3.1.2) (0.6.2) Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from donkeycar==3.1.2) (4.5.3) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from donkeycar==3.1.2) (2.23.0) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from donkeycar==3.1.2) (2.10.0) Requirement already satisfied: moviepy in /usr/local/lib/python3.6/dist-packages (from donkeycar==3.1.2) (0.2.3.5) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from donkeycar==3.1.2) (1.0.3) Requirement already satisfied: PrettyTable in /usr/local/lib/python3.6/dist-packages (from donkeycar==3.1.2) (0.7.2) Collecting paho-mqtt Downloading https://files.pythonhosted.org/packages/59/11/1dd5c70f0f27a88a3a05772cd95f6087ac479fac66d9c7752ee5e16ddbbc/paho-mqtt-1.5.0.tar.gz (99kB) |████████████████████████████████| 102kB 3.4MB/s Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from donkeycar==3.1.2) (3.2.1) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;donkeycar==3.1.2) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;donkeycar==3.1.2) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;donkeycar==3.1.2) (2020.4.5.1) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;donkeycar==3.1.2) (2.9) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py-&gt;donkeycar==3.1.2) (1.12.0) Requirement already satisfied: imageio&lt;3.0,&gt;=2.1.2 in /usr/local/lib/python3.6/dist-packages (from moviepy-&gt;donkeycar==3.1.2) (2.4.1) Requirement already satisfied: tqdm&lt;5.0,&gt;=4.11.2 in /usr/local/lib/python3.6/dist-packages (from moviepy-&gt;donkeycar==3.1.2) (4.41.1) Requirement already satisfied: decorator&lt;5.0,&gt;=4.0.2 in /usr/local/lib/python3.6/dist-packages (from moviepy-&gt;donkeycar==3.1.2) (4.4.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;donkeycar==3.1.2) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;donkeycar==3.1.2) (2.8.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;donkeycar==3.1.2) (1.2.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;donkeycar==3.1.2) (0.10.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;donkeycar==3.1.2) (2.4.7) Building wheels for collected packages: paho-mqtt Building wheel for paho-mqtt (setup.py) ... done Created wheel for paho-mqtt: filename=paho_mqtt-1.5.0-cp36-none-any.whl size=61416 sha256=361839e2e72f674579aacf767c1756b47c4de8dcf28e6e77ecc0f1d13b6a6727 Stored in directory: /root/.cache/pip/wheels/02/94/6c/8474137cb7a5a3e001d70a22c8ff919caee69435376bccce79 Successfully built paho-mqtt Installing collected packages: paho-mqtt, donkeycar Running setup.py develop for donkeycar Successfully installed donkeycar paho-mqtt-1.5.0 . Create Project . In this step the following actions take place . create necessary folders (models, data, logs) | copying necessary files into folders (manage.py, myconfig.py etc.) | . !donkey createcar --path /content/mycar . using donkey v3.1.2 ... Creating car folder: /content/mycar making dir /content/mycar Creating data &amp; model folders. making dir /content/mycar/models making dir /content/mycar/data making dir /content/mycar/logs Copying car application template: complete Copying car config defaults. Adjust these before starting your car. Copying train script. Adjust these before starting your car. Copying my car config overrides Donkey setup complete. . Step 2: Supply Data . In order to train the neural network we need to supply trainings data which are recorded on the raspi during driving the donkeycar on the track . Step 2 opt A: Supply own data . If you have own data proceed here, if you want to use data which were made available via GitHub please continue to (link only works in blog not in Colab) section Supply GitHub hosted data . Zip data on raspi . Copy the following code and run on raspi . Note: Copying of the data is much faster if the data is zipped to one file. . cd ~/mycar/data # either compress just one folder tar -czf tub_xx_yyyy_mm_dd.tar.gz tub_xx_yyyy_mm_dd # or all folders starting with &quot;tub&quot; tar -czf trainingsData2020_03-01.tar.gz tub* . This will create a tub_xx_yyyy_mm_dd.tar.gz file under ~/mycar/data . Copy the zipped tub to your local PC . Run this on your local pc if you are using linux/mac . sftp pi@raspberry.local cd ~/mycar/data get tub_xx_yyyy_mm_dd.tar.gz . If you are on a windows, download sftp utility like filezilla or putty . Define your tub name here . tub_name=&quot;tubVaihingenIIICleaned200126&quot; . Upload the tub from Google Drive . First upload the tub_x_yyyy_mm_dd.tar.gz to Google Drive. We will then mount Google Drive from colab and copy the data from Drive directly. . Note: To copy data from Google Drive to Colab is faster than uploading it from local machine. When you run the cell below, you will need to click the link and generate an authorization code to for colab to access your drive. . drive.mount(&#39;/content/drive&#39;) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly Enter your authorization code: ·········· Mounted at /content/drive . Suppose you upload the tub_xx_yyyy_mm_dd.tar.gz to Google Drive/mycar/tub_xx_yyyy_mm_dd.tar.gz, this is how you copy it from Google Drive to colab . %cd /content/mycar !rm -rf data !mkdir data %cd /content/mycar/data !cp /content/drive/My Drive/myCar/{tub_name}.tar.gz . . /content/mycar /content/mycar/data . And untar it to the right place . %cd /content/mycar/data !tar -xzf {tub_name}.tar.gz . /content/mycar/data . Lets look at one image to see we got valid data . %cd /content/mycar/data/tubVaihingenIIICleaned200126/ file = glob.glob(&quot;*.jpg&quot;) Image(file[100]) . /content/mycar/data/tubVaihingenIIICleaned200126 . Check quality of data . You want data which has left and right turns preferably in equal shares. A histogram is a good tool to check if this is the case. You can use a donkeycar tool for that . donkey tubhist &lt;tub_path&gt; --rec=&lt;&quot;user/angle&quot;&gt; . The histogram shows that mainly the car drove straight ahead and more left turns than right turns. It is good practice to drive a course clock wise and anti clock wise to avoid this imbalance. . %cd /content/mycar !donkey tubhist --tub data/{tub_name} --rec=&quot;user/angle&quot; file = glob.glob(&quot;tubVaihingenIIICleaned200126_hist_user_angle.png&quot;) Image(file[0]) . /content/mycar using donkey v3.1.2 ... TubGroup:tubpaths: [&#39;/content/mycar/data/tubVaihingenIIICleaned200126&#39;] joining the tubs 14234 records together. This could take 0 minutes. saving image to: tubVaihingenIIICleaned200126_hist_user_angle.png &lt;Figure size 640x480 with 1 Axes&gt; . Next step is to train your model in section Upload local files (Link only works in blog not in Colab) . Supply GitHub hosted data . If you don&#39;t have own training data you might want to use an example data set . Note: The training data is cleaned (tubclean) but whether or not you get a good working model out of it... See instructions below how to get the data into the Colab environment . The training data are hosted on GitHub . clone GitHub repo | move file to data folder | unzip file | . Step 2 opt B: Use data from RoboCarEsslingen . The first data set we use is from RoboCar Esslingen GitHub which is operated by the meetup group Esslinger Makerspace Projekt: Autonomen RoboCar bauen located in Esslingen, Germany. The data was recorded at #8.7 Quarterly Hack: DIYrobocars Build &amp; Race in Suttgart with kind support of Bosch. If you want to use data from Connected Atonomous Mobility proceed in chapter (link only works in blog not in Colab) section Use data from Connected Autonomous Mobility . %cd /content/mycar !rm -rf data !mkdir data %cd /content/mycar/data !pwd ! git clone --recursive https://github.com/RoboCarEsslingen/trainingData.git . Move zip file to data folder and unzip . shutil.move(&quot;/content/mycar/data/trainingData/tubVaihingenIIICleaned200126.tar.gz&quot;, &quot;/content/mycar/data&quot;) . Unzip the training data file . %cd /content/mycar/data !tar -xzf tubVaihingenIIICleaned200126.tar.gz . Check quality of data . You want data which has left and right turns preferably in equal shares. A histogram is a good tool to check if this is the case. You can use a donkeycar tool for that . donkey tubhist &lt;tub_path&gt; --rec=&lt;&quot;user/angle&quot;&gt; . The histogram shows that mainly the car drove straight ahead and more left turns than right turns. It is good practice to drive a course clock wise and anti clock wise to avoid this imbalance. . %cd /content/mycar !donkey tubhist --tub data/{tub_name} --rec=&quot;user/angle&quot; file = glob.glob(&quot;tubVaihingenIIICleaned200126_hist_user_angle.png&quot;) Image(file[0]) . Use data from Connected Autonomous Mobility . Another data set is from Connected Autonomous Mobility We clone the whole repo because I don&#39;t know how to download a single file from a GitHub repo, if you know how to this than please leave a note the comment section. . %cd /content/mycar !rm -rf data !mkdir data %cd /content/mycar/data !pwd ! git clone --recursive https://github.com/connected-autonomous-mobility/20-data . Move zip file to data folder and unzip . import shutil shutil.move(&quot;/content/mycar/data/20-data/20190414-BOSCH-Solaris-Course/tub_36_19-04-13.zip&quot;, &quot;/content/mycar/data&quot;) . Unzip the training data file . %cd /content/mycar/data !unzip tub_36_19-04-13.zip . Lets look at one image to get an impression what the car saw. . # This is formatted as code . %cd /content/mycar/data/tub_36_19-04-13/ file = glob.glob(&quot;*.jpg&quot;) Image(file[300]) . Check quality of data . You want data which has left and right turns preferably in equal shares. A histogram is a good tool to check if this is the case. You can use a donkeycar tool for that . donkey tubhist &lt;tub_path&gt; --rec=&lt;&quot;user/angle&quot;&gt; . The histogram shows that mainly the car drove straight ahead, left and right turns are pretty well balanced . tub_name=&quot;tub_36_19-04-13&quot; %cd /content/mycar !donkey tubhist --tub data/{tub_name} --rec=&quot;user/angle&quot; file = glob.glob(&quot;tub_36_19-04-13_hist_user_angle.png&quot;) Image(file[0]) . Step 3: Upload myconfig.py . You can upload files from local machine as well, but probably is slower than above approach downloading files from Google Drive . . Get myconfig.py . The file myconfig.py has to be the identical during training and driving, therefore it makes sense to upload the myconfig.py which you are using on the car. . Note: In myconfig.py there are parameters which control the training such as: . line parameter --type to the python manage.py train and drive commands. DEFAULT_MODEL_TYPE = &#39;linear&#39; (linear|categorical|rnn|imu|behavior|3d|localizer|latent) BATCH_SIZE = 128 how many records to use when doing one pass of gradient decent. Use a smaller number if your gpu is running out of memory. TRAIN_TEST_SPLIT = 0.8 what percent of records to use for training. the remaining used for validation. MAX_EPOCHS = 100 how many times to visit all records of your data SHOW_PLOT = True would you like to see a pop up display of final loss? VEBOSE_TRAIN = True would you like to see a progress bar with text during training? USE_EARLY_STOP = True would you like to stop the training if we see it&#39;s not improving fit? EARLY_STOP_PATIENCE = 5 how many epochs to wait before no improvement MIN_DELTA = .0005 early stop will want this much loss change before calling it improved. PRINT_MODEL_SUMMARY = True print layers and weights to stdout OPTIMIZER = None adam, sgd, rmsprop, etc.. None accepts default LEARNING_RATE = 0.001 only used when OPTIMIZER specified LEARNING_RATE_DECAY = 0.0 only used when OPTIMIZER specified SEND_BEST_MODEL_TO_PI = False change to true to automatically send best model during training CACHE_IMAGES = True keep images in memory. will speed successive epochs, but crater if not enough mem. PRUNE_CNN = False This will remove weights from your model. The primary goal is to increase performance. PRUNE_PERCENT_TARGET = 75 The desired percentage of pruning. PRUNE_PERCENT_PER_ITERATION = 20 Percentage of pruning that is perform per iteration. PRUNE_VAL_LOSS_DEGRADATION_LIMIT = 0.2 The max amount of validation loss that is permitted during pruning. PRUNE_EVAL_PERCENT_OF_DATASET = .05 percent of dataset used to perform evaluation of model. RNN or 3D SEQUENCE_LENGTH = 3 #some models use a number of images over time. This controls how many. # # Region of interest cropping # # only supported in Categorical and Linear models. ROI_CROP_TOP = 0 #the number of rows of pixels to ignore on the top of the image ROI_CROP_BOTTOM = 0 #the number of rows of pixels to ignore on the bottom of the image . %cd /content/mycar !cp /content/drive/My Drive/myCar/myconfig.py . . /content/mycar . Step 4: Train your model . There are several types of modes available: . linear | categorical | rnn | imu | behavior | 3d | localizer And you can use pre-trained models by adding a flag [--transfer=&lt;model&gt;] | . | . Step 4 opt A: Transfer learning using pre-trained model . . Note: You can use a pre-trained model and use transfer learning . Do not forget to set the variables in myconfig.py . FREEZE_LAYERS = True `#default False will allow all layers to be modified by training NUM_LAST_LAYERS_TO_TRAIN = 7 `#when freezing layers, how many layers from the last should be allowed to train? . Upload pre-trained model . Upload model in case you want to use a pre-trained model for transfer learning. To define which layers shall be trained and which shall be frozen set the parameters in `myconfig.py`` . Model transfer options . When copying weights during a model transfer operation, should we freeze a certain number of layers to the incoming weights and not allow them to change during training? . FREEZE_LAYERS = False #default False will allow all layers to be modified by training NUM_LAST_LAYERS_TO_TRAIN = 7 #when freezing layers, how many layers from the last should be allowed to train? . %cd /content/mycar/models !cp /content/drive/My Drive/myCar/base_linear.h5 . . /content/mycar/models . Plot the model structure . from tensorflow.keras.utils import plot_model from tensorflow.keras.models import load_model %cd /content/mycar/models model = load_model(&#39;base_linear.h5&#39;) plot_model( model, to_file=&quot;model.png&quot;, show_shapes=False, show_layer_names=True, rankdir=&quot;TB&quot;, expand_nested=False, dpi=96, ) . /content/mycar/models WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. . Start transfer learning of pre-trained model . Use the manage.py script to start training . !python /content/mycar/manage.py train --type=linear --transfer=/content/mycar/models/base_linear.h5 --model=/content/mycar/models/mypilot.h5 . using donkey v3.1.2 ... loading config file: /content/mycar/config.py myconfig myconfig.py loading personal config over-rides from myconfig.py config loaded &#34;get_model_by_type&#34; model Type is: linear WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. training with model type &lt;class &#39;donkeycar.parts.keras.KerasLinear&#39;&gt; loading weights from model /content/mycar/models/base_linear.h5 WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor 2020-05-28 15:11:01.559596: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1 2020-05-28 15:11:01.562497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:11:01.563191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285 pciBusID: 0000:00:04.0 2020-05-28 15:11:01.563474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-28 15:11:01.565174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-28 15:11:01.566931: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-05-28 15:11:01.567244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-05-28 15:11:01.568968: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-05-28 15:11:01.570096: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-05-28 15:11:01.574256: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-05-28 15:11:01.574380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:11:01.575112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:11:01.575708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0 2020-05-28 15:11:01.581927: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz 2020-05-28 15:11:01.582168: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x67cee00 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-05-28 15:11:01.582199: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-05-28 15:11:01.673210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:11:01.673930: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x67cefc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-05-28 15:11:01.673963: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0 2020-05-28 15:11:01.674127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:11:01.674884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285 pciBusID: 0000:00:04.0 2020-05-28 15:11:01.674977: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-28 15:11:01.675006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-28 15:11:01.675028: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-05-28 15:11:01.675051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-05-28 15:11:01.675070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-05-28 15:11:01.675089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-05-28 15:11:01.675109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-05-28 15:11:01.675183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:11:01.675764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:11:01.676337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0 2020-05-28 15:11:01.676400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-28 15:11:01.677577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-05-28 15:11:01.677605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186] 0 2020-05-28 15:11:01.677616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0: N 2020-05-28 15:11:01.677718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:11:01.678342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:11:01.678954: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0. 2020-05-28 15:11:01.678998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14858 MB memory) -&gt; physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0) Model: &#34;model&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== img_in (InputLayer) [(None, 120, 160, 3) 0 __________________________________________________________________________________________________ cropping2d (Cropping2D) (None, 120, 160, 3) 0 img_in[0][0] __________________________________________________________________________________________________ batch_normalization_v1 (BatchNo (None, 120, 160, 3) 12 cropping2d[0][0] __________________________________________________________________________________________________ conv2d_1 (Conv2D) (None, 58, 78, 24) 1824 batch_normalization_v1[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 58, 78, 24) 0 conv2d_1[0][0] __________________________________________________________________________________________________ conv2d_2 (Conv2D) (None, 27, 37, 32) 19232 dropout[0][0] __________________________________________________________________________________________________ dropout_1 (Dropout) (None, 27, 37, 32) 0 conv2d_2[0][0] __________________________________________________________________________________________________ conv2d_3 (Conv2D) (None, 12, 17, 64) 51264 dropout_1[0][0] __________________________________________________________________________________________________ dropout_2 (Dropout) (None, 12, 17, 64) 0 conv2d_3[0][0] __________________________________________________________________________________________________ conv2d_4 (Conv2D) (None, 10, 15, 64) 36928 dropout_2[0][0] __________________________________________________________________________________________________ dropout_3 (Dropout) (None, 10, 15, 64) 0 conv2d_4[0][0] __________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 8, 13, 64) 36928 dropout_3[0][0] __________________________________________________________________________________________________ dropout_4 (Dropout) (None, 8, 13, 64) 0 conv2d_5[0][0] __________________________________________________________________________________________________ flattened (Flatten) (None, 6656) 0 dropout_4[0][0] __________________________________________________________________________________________________ dense (Dense) (None, 100) 665700 flattened[0][0] __________________________________________________________________________________________________ dropout_5 (Dropout) (None, 100) 0 dense[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 50) 5050 dropout_5[0][0] __________________________________________________________________________________________________ dropout_6 (Dropout) (None, 50) 0 dense_1[0][0] __________________________________________________________________________________________________ n_outputs0 (Dense) (None, 1) 51 dropout_6[0][0] __________________________________________________________________________________________________ n_outputs1 (Dense) (None, 1) 51 dropout_6[0][0] ================================================================================================== Total params: 817,040 Trainable params: 817,034 Non-trainable params: 6 __________________________________________________________________________________________________ None found 0 pickles writing json records and images in tub /content/mycar/data/tubVaihingenIIICleaned200126 /content/mycar/data/tubVaihingenIIICleaned200126 collating 14234 records ... train: 11387, val: 2847 total records: 14234 steps_per_epoch 177 Epoch 1/100 2020-05-28 15:11:04.890586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-28 15:11:06.394756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 176/177 [============================&gt;.] - ETA: 0s - loss: 0.3672 - n_outputs0_loss: 0.2321 - n_outputs1_loss: 0.1352Epoch 1/100 44/177 [======&gt;.......................] - ETA: 6s - loss: 0.6511 - n_outputs0_loss: 0.6171 - n_outputs1_loss: 0.0341 Epoch 00001: val_loss improved from inf to 0.65115, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 19s 106ms/step - loss: 0.3660 - n_outputs0_loss: 0.2315 - n_outputs1_loss: 0.1345 - val_loss: 0.6511 - val_n_outputs0_loss: 0.6171 - val_n_outputs1_loss: 0.0341 Epoch 2/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1948 - n_outputs0_loss: 0.1780 - n_outputs1_loss: 0.0169Epoch 1/100 40/177 [=====&gt;........................] - ETA: 1s - loss: 0.6370 - n_outputs0_loss: 0.6252 - n_outputs1_loss: 0.0119 Epoch 00002: val_loss improved from 0.65115 to 0.63501, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 27ms/step - loss: 0.1954 - n_outputs0_loss: 0.1785 - n_outputs1_loss: 0.0168 - val_loss: 0.6350 - val_n_outputs0_loss: 0.6229 - val_n_outputs1_loss: 0.0122 Epoch 3/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1780 - n_outputs0_loss: 0.1649 - n_outputs1_loss: 0.0131Epoch 1/100 42/177 [======&gt;.......................] - ETA: 1s - loss: 0.6386 - n_outputs0_loss: 0.6316 - n_outputs1_loss: 0.0070 Epoch 00003: val_loss did not improve from 0.63501 177/177 [==============================] - 4s 25ms/step - loss: 0.1778 - n_outputs0_loss: 0.1647 - n_outputs1_loss: 0.0131 - val_loss: 0.6390 - val_n_outputs0_loss: 0.6322 - val_n_outputs1_loss: 0.0068 Epoch 4/100 174/177 [============================&gt;.] - ETA: 0s - loss: 0.1637 - n_outputs0_loss: 0.1516 - n_outputs1_loss: 0.0120Epoch 1/100 44/177 [======&gt;.......................] - ETA: 1s - loss: 0.6309 - n_outputs0_loss: 0.6175 - n_outputs1_loss: 0.0133 Epoch 00004: val_loss improved from 0.63501 to 0.63087, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 26ms/step - loss: 0.1635 - n_outputs0_loss: 0.1515 - n_outputs1_loss: 0.0120 - val_loss: 0.6309 - val_n_outputs0_loss: 0.6175 - val_n_outputs1_loss: 0.0133 Epoch 5/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1544 - n_outputs0_loss: 0.1429 - n_outputs1_loss: 0.0115Epoch 1/100 42/177 [======&gt;.......................] - ETA: 1s - loss: 0.3351 - n_outputs0_loss: 0.3270 - n_outputs1_loss: 0.0081 Epoch 00005: val_loss improved from 0.63087 to 0.34012, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 27ms/step - loss: 0.1543 - n_outputs0_loss: 0.1428 - n_outputs1_loss: 0.0115 - val_loss: 0.3401 - val_n_outputs0_loss: 0.3315 - val_n_outputs1_loss: 0.0086 Epoch 6/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1488 - n_outputs0_loss: 0.1371 - n_outputs1_loss: 0.0117Epoch 1/100 43/177 [======&gt;.......................] - ETA: 1s - loss: 0.1704 - n_outputs0_loss: 0.1677 - n_outputs1_loss: 0.0027 Epoch 00006: val_loss improved from 0.34012 to 0.16988, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 26ms/step - loss: 0.1489 - n_outputs0_loss: 0.1372 - n_outputs1_loss: 0.0117 - val_loss: 0.1699 - val_n_outputs0_loss: 0.1673 - val_n_outputs1_loss: 0.0026 Epoch 7/100 174/177 [============================&gt;.] - ETA: 0s - loss: 0.1403 - n_outputs0_loss: 0.1290 - n_outputs1_loss: 0.0113Epoch 1/100 42/177 [======&gt;.......................] - ETA: 1s - loss: 0.1439 - n_outputs0_loss: 0.1418 - n_outputs1_loss: 0.0021 Epoch 00007: val_loss improved from 0.16988 to 0.14487, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 26ms/step - loss: 0.1403 - n_outputs0_loss: 0.1290 - n_outputs1_loss: 0.0113 - val_loss: 0.1449 - val_n_outputs0_loss: 0.1429 - val_n_outputs1_loss: 0.0020 Epoch 8/100 174/177 [============================&gt;.] - ETA: 0s - loss: 0.1326 - n_outputs0_loss: 0.1223 - n_outputs1_loss: 0.0103Epoch 1/100 42/177 [======&gt;.......................] - ETA: 1s - loss: 0.1371 - n_outputs0_loss: 0.1334 - n_outputs1_loss: 0.0037 Epoch 00008: val_loss improved from 0.14487 to 0.13729, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 26ms/step - loss: 0.1325 - n_outputs0_loss: 0.1221 - n_outputs1_loss: 0.0104 - val_loss: 0.1373 - val_n_outputs0_loss: 0.1337 - val_n_outputs1_loss: 0.0036 Epoch 9/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1266 - n_outputs0_loss: 0.1164 - n_outputs1_loss: 0.0102Epoch 1/100 41/177 [=====&gt;........................] - ETA: 1s - loss: 0.1297 - n_outputs0_loss: 0.1279 - n_outputs1_loss: 0.0018 Epoch 00009: val_loss improved from 0.13729 to 0.12960, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 27ms/step - loss: 0.1267 - n_outputs0_loss: 0.1163 - n_outputs1_loss: 0.0103 - val_loss: 0.1296 - val_n_outputs0_loss: 0.1277 - val_n_outputs1_loss: 0.0018 Epoch 10/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1211 - n_outputs0_loss: 0.1109 - n_outputs1_loss: 0.0101Epoch 1/100 42/177 [======&gt;.......................] - ETA: 1s - loss: 0.1316 - n_outputs0_loss: 0.1299 - n_outputs1_loss: 0.0017 Epoch 00010: val_loss did not improve from 0.12960 177/177 [==============================] - 4s 25ms/step - loss: 0.1208 - n_outputs0_loss: 0.1107 - n_outputs1_loss: 0.0101 - val_loss: 0.1326 - val_n_outputs0_loss: 0.1309 - val_n_outputs1_loss: 0.0017 Epoch 11/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1195 - n_outputs0_loss: 0.1088 - n_outputs1_loss: 0.0107Epoch 1/100 42/177 [======&gt;.......................] - ETA: 1s - loss: 0.1345 - n_outputs0_loss: 0.1311 - n_outputs1_loss: 0.0035 Epoch 00011: val_loss did not improve from 0.12960 177/177 [==============================] - 4s 25ms/step - loss: 0.1195 - n_outputs0_loss: 0.1087 - n_outputs1_loss: 0.0107 - val_loss: 0.1341 - val_n_outputs0_loss: 0.1308 - val_n_outputs1_loss: 0.0034 Epoch 12/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1116 - n_outputs0_loss: 0.1021 - n_outputs1_loss: 0.0095Epoch 1/100 39/177 [=====&gt;........................] - ETA: 1s - loss: 0.1315 - n_outputs0_loss: 0.1284 - n_outputs1_loss: 0.0031 Epoch 00012: val_loss improved from 0.12960 to 0.12898, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 26ms/step - loss: 0.1120 - n_outputs0_loss: 0.1025 - n_outputs1_loss: 0.0095 - val_loss: 0.1290 - val_n_outputs0_loss: 0.1260 - val_n_outputs1_loss: 0.0029 Epoch 13/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1108 - n_outputs0_loss: 0.1016 - n_outputs1_loss: 0.0092Epoch 1/100 42/177 [======&gt;.......................] - ETA: 1s - loss: 0.1257 - n_outputs0_loss: 0.1231 - n_outputs1_loss: 0.0026 Epoch 00013: val_loss improved from 0.12898 to 0.12570, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 26ms/step - loss: 0.1106 - n_outputs0_loss: 0.1014 - n_outputs1_loss: 0.0092 - val_loss: 0.1257 - val_n_outputs0_loss: 0.1232 - val_n_outputs1_loss: 0.0025 Epoch 14/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1060 - n_outputs0_loss: 0.0964 - n_outputs1_loss: 0.0096Epoch 1/100 44/177 [======&gt;.......................] - ETA: 1s - loss: 0.1357 - n_outputs0_loss: 0.1313 - n_outputs1_loss: 0.0044 Epoch 00014: val_loss did not improve from 0.12570 177/177 [==============================] - 4s 25ms/step - loss: 0.1062 - n_outputs0_loss: 0.0966 - n_outputs1_loss: 0.0096 - val_loss: 0.1357 - val_n_outputs0_loss: 0.1313 - val_n_outputs1_loss: 0.0044 Epoch 15/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1013 - n_outputs0_loss: 0.0920 - n_outputs1_loss: 0.0093Epoch 1/100 41/177 [=====&gt;........................] - ETA: 1s - loss: 0.1331 - n_outputs0_loss: 0.1314 - n_outputs1_loss: 0.0016 Epoch 00015: val_loss did not improve from 0.12570 177/177 [==============================] - 4s 25ms/step - loss: 0.1012 - n_outputs0_loss: 0.0918 - n_outputs1_loss: 0.0093 - val_loss: 0.1316 - val_n_outputs0_loss: 0.1297 - val_n_outputs1_loss: 0.0019 Epoch 16/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.1000 - n_outputs0_loss: 0.0913 - n_outputs1_loss: 0.0088Epoch 1/100 44/177 [======&gt;.......................] - ETA: 1s - loss: 0.1233 - n_outputs0_loss: 0.1207 - n_outputs1_loss: 0.0026 Epoch 00016: val_loss improved from 0.12570 to 0.12330, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 26ms/step - loss: 0.1001 - n_outputs0_loss: 0.0914 - n_outputs1_loss: 0.0087 - val_loss: 0.1233 - val_n_outputs0_loss: 0.1207 - val_n_outputs1_loss: 0.0026 Epoch 17/100 174/177 [============================&gt;.] - ETA: 0s - loss: 0.0931 - n_outputs0_loss: 0.0847 - n_outputs1_loss: 0.0084Epoch 1/100 42/177 [======&gt;.......................] - ETA: 1s - loss: 0.1167 - n_outputs0_loss: 0.1142 - n_outputs1_loss: 0.0025 Epoch 00017: val_loss improved from 0.12330 to 0.11700, saving model to /content/mycar/models/mypilot.h5 177/177 [==============================] - 5s 27ms/step - loss: 0.0932 - n_outputs0_loss: 0.0848 - n_outputs1_loss: 0.0084 - val_loss: 0.1170 - val_n_outputs0_loss: 0.1138 - val_n_outputs1_loss: 0.0032 Epoch 18/100 176/177 [============================&gt;.] - ETA: 0s - loss: 0.0894 - n_outputs0_loss: 0.0811 - n_outputs1_loss: 0.0083Epoch 1/100 42/177 [======&gt;.......................] - ETA: 1s - loss: 0.1195 - n_outputs0_loss: 0.1167 - n_outputs1_loss: 0.0027 Epoch 00018: val_loss did not improve from 0.11700 177/177 [==============================] - 4s 25ms/step - loss: 0.0894 - n_outputs0_loss: 0.0811 - n_outputs1_loss: 0.0083 - val_loss: 0.1210 - val_n_outputs0_loss: 0.1184 - val_n_outputs1_loss: 0.0026 Epoch 19/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.0888 - n_outputs0_loss: 0.0805 - n_outputs1_loss: 0.0082Epoch 1/100 44/177 [======&gt;.......................] - ETA: 1s - loss: 0.1193 - n_outputs0_loss: 0.1180 - n_outputs1_loss: 0.0014 Epoch 00019: val_loss did not improve from 0.11700 177/177 [==============================] - 4s 25ms/step - loss: 0.0889 - n_outputs0_loss: 0.0805 - n_outputs1_loss: 0.0084 - val_loss: 0.1193 - val_n_outputs0_loss: 0.1180 - val_n_outputs1_loss: 0.0014 Epoch 20/100 175/177 [============================&gt;.] - ETA: 0s - loss: 0.0861 - n_outputs0_loss: 0.0780 - n_outputs1_loss: 0.0081Epoch 1/100 44/177 [======&gt;.......................] - ETA: 1s - loss: 0.1262 - n_outputs0_loss: 0.1225 - n_outputs1_loss: 0.0037 Epoch 00020: val_loss did not improve from 0.11700 177/177 [==============================] - 4s 25ms/step - loss: 0.0862 - n_outputs0_loss: 0.0782 - n_outputs1_loss: 0.0081 - val_loss: 0.1262 - val_n_outputs0_loss: 0.1225 - val_n_outputs1_loss: 0.0037 Epoch 21/100 174/177 [============================&gt;.] - ETA: 0s - loss: 0.0812 - n_outputs0_loss: 0.0735 - n_outputs1_loss: 0.0077Epoch 1/100 41/177 [=====&gt;........................] - ETA: 1s - loss: 0.1191 - n_outputs0_loss: 0.1174 - n_outputs1_loss: 0.0017 Epoch 00021: val_loss did not improve from 0.11700 177/177 [==============================] - 4s 25ms/step - loss: 0.0817 - n_outputs0_loss: 0.0738 - n_outputs1_loss: 0.0079 - val_loss: 0.1185 - val_n_outputs0_loss: 0.1161 - val_n_outputs1_loss: 0.0024 Epoch 22/100 174/177 [============================&gt;.] - ETA: 0s - loss: 0.0813 - n_outputs0_loss: 0.0734 - n_outputs1_loss: 0.0079Epoch 1/100 44/177 [======&gt;.......................] - ETA: 1s - loss: 0.1172 - n_outputs0_loss: 0.1146 - n_outputs1_loss: 0.0025 Epoch 00022: val_loss did not improve from 0.11700 177/177 [==============================] - 4s 25ms/step - loss: 0.0810 - n_outputs0_loss: 0.0732 - n_outputs1_loss: 0.0078 - val_loss: 0.1172 - val_n_outputs0_loss: 0.1146 - val_n_outputs1_loss: 0.0025 Epoch 00022: early stopping Training completed in 0:01:55. -- Best Eval Loss :0.116997 &lt;Figure size 640x480 with 1 Axes&gt; . Step 4 opt B: Train RNN model . The RNN model combines several images to calculate steering and throttle. Use the manage.py script to start training . !python /content/mycar/manage.py train --type rnn --model /content/mycar/models/mypilot.h5 --aug . Step 5: Check model and transfer data . To check the quality of the model we look at the loss curve and see how well commanded and predicted steering and throttle values match. We transfer the data to the car and show how to start the self driving car. . Plot loss curve of model . The curve should show smaller loss vs epochs and the train and validation loss should not differ too much. . Tip: If train loss is much smaller than validation loss your model might be overfitting. . %cd /content/mycar/models file = glob.glob(&quot;*.png&quot;) Image(file[0]) . /content/mycar/models . Plot commands and predictions . You can use . donkey tubplot &lt;tub_path&gt; [--model=&lt;model_path&gt;] . to plot the commands and predictions of steering and throttle . %cd /content/mycar !donkey tubplot --tub=data/ --tub=data/tubVaihingenIIICleaned200126 --model=models/mypilot.h5 file = glob.glob(&quot;/content/mycar/models/mypilot.h5_pred.png&quot;) Image(file[0]) . /content/mycar using donkey v3.1.2 ... loading config file: ./config.py myconfig myconfig.py loading personal config over-rides from myconfig.py config loaded &#34;get_model_by_type&#34; model Type is: linear WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor 2020-05-28 15:15:21.333668: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1 2020-05-28 15:15:21.336208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:15:21.336818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285 pciBusID: 0000:00:04.0 2020-05-28 15:15:21.337135: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-28 15:15:21.339213: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-28 15:15:21.340908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-05-28 15:15:21.341246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-05-28 15:15:21.352124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-05-28 15:15:21.355539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-05-28 15:15:21.362433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-05-28 15:15:21.362552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:15:21.363208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:15:21.363746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0 2020-05-28 15:15:21.368965: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz 2020-05-28 15:15:21.369168: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x61aee00 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-05-28 15:15:21.369195: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-05-28 15:15:21.459586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:15:21.460235: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x61aefc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-05-28 15:15:21.460266: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0 2020-05-28 15:15:21.460429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:15:21.460998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285 pciBusID: 0000:00:04.0 2020-05-28 15:15:21.461059: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-28 15:15:21.461084: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-28 15:15:21.461104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10 2020-05-28 15:15:21.461125: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10 2020-05-28 15:15:21.461143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10 2020-05-28 15:15:21.461159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10 2020-05-28 15:15:21.461178: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2020-05-28 15:15:21.461246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:15:21.462030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:15:21.462513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0 2020-05-28 15:15:21.462570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1 2020-05-28 15:15:21.463567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-05-28 15:15:21.463592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186] 0 2020-05-28 15:15:21.463603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0: N 2020-05-28 15:15:21.463726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:15:21.464301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2020-05-28 15:15:21.464825: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0. 2020-05-28 15:15:21.464864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14858 MB memory) -&gt; physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0) processing 1000 records: 2020-05-28 15:15:22.373437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10 2020-05-28 15:15:22.622740: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 &lt;Figure size 640x480 with 2 Axes&gt; . Copy the trained model back to Donkey Car (Pi) . Once the training is complete on Colab, download . mypilot.h5 file from /content/mycar/models/ | myconfig.py file from /content/mycar/ | . . files.download(&#39;./mypilot.h5&#39;) %cd /content/mycar files.download(&#39;myconfig.py&#39;) . Alternatively, you can copy the model back to Google Drive too . !cp /content/mycar/models/mypilot.h5 /content/drive/My Drive/myCar/mypilot.h5 . Copy the file from your PC or Mac to the Raspberry Pi using Filezilla or scp command. . sftp pi@raspberry.local cd mycar/models put mypilot.h5 . Start Autopilot on Pi . cd ~/mycar python manage.py drive --model models/mypilot.h5 --js . Step 6: Bonus - Salient Object Visualization . The salient visualization gives an indication which parts of the image caused the highest activations in the model. Its a good method to understand what triggers the steering and indentify problems . reflections | distractions off the track . Note: It seems like the salient mode doesn&#8217;t work for RNN networks | . !pip uninstall keras-vis !pip install git+https://github.com/sctse999/keras-vis . %cd /content/mycar !donkey makemovie --tub data/{tub_name} --model models/mypilot.h5 --type linear --salient . Download the movie to local machine . %cd /content/mycar !ls -ahl files.download(&#39;tub_movie.mp4&#39;) . Or download the file to Google Drive . !cp /content/mycar/tub_movie.mp4 /content/drive/My Drive/myCar/tub_movie.mp4 .",
            "url": "https://uwesterr.github.io/MlFastAiBlogNew/donkeycar/colab/2020/03/01/TrainDonkeyCar.html",
            "relUrl": "/donkeycar/colab/2020/03/01/TrainDonkeyCar.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Colabguide",
            "content": "Working with fastpages, GitHub and Colab . . The idea is to work with Jupyter Notebook in Colab, the notebook is part of a Fastpages blog and should be edited either in Colab or VS Code . write a Blog in Fastpages | format of Blog is Jupyter Notebook | run the Notebook in Colab | also edit in VS Code is necessary use Git to synchronize GitHub repo with VS Code local copy | . | . Write a blog with Fastpages . A detailed instruction on how to write Fastpage blogs is given at . for juypter notebooks | for markdown | . Working with notebook in Colab . Once the blog is opened there are buttons . View on GitHub The Jupyter Notbeook file is opened in the GitHub environment | Open in Colab The Jupyter Notbeook file is opened in the Colab environment 1 | . In the Colab environment the file can be run with GPU support. An intro to Colab is given here . To save the changes chose under “File” the option “Save a copy in GitHub” as shown below . . . . Colab will open a dialog as shown below It might be that it asks for your GitHub credentials before opening the dialog . . . . Working with VS Code . To work with the Fastpages blog clone the repository as described here . It is then easy to synchronize with GitHub, note, don’t forget to pull the repo once you saved a notebook version from Colab to GitHub. . Avoid conflicting versions . If you do the following . Change file in Colab | Save to GitHub | Change same file in VS | Try to push | . You will have *conflicts in the file** and need command line magic to solve the issue. I ended up setting the whole blog up from scratch. . Better: use Git: Snyc which does a Git: Pull first and then a Git: Push . Anybody can open a copy of any github-hosted notebook within Colab just add for markup [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb) or for html &lt;a href=&quot;https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb&quot;&gt; &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge. svg&quot; alt=&quot;Open In Colab&quot;/&gt; &lt;/a&gt; &#8617; . |",
            "url": "https://uwesterr.github.io/MlFastAiBlogNew/fastpages/github/jupyter%20notebook/vs%20code/colab/2020/03/01/ColabGuide.html",
            "relUrl": "/fastpages/github/jupyter%20notebook/vs%20code/colab/2020/03/01/ColabGuide.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://uwesterr.github.io/MlFastAiBlogNew/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://uwesterr.github.io/MlFastAiBlogNew/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://uwesterr.github.io/MlFastAiBlogNew/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://uwesterr.github.io/MlFastAiBlogNew/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}